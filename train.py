import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from scipy.io import arff
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import CategoricalNB

# === 1. Load dá»¯ liá»‡u tá»« file ARFF ===
def load_data(filepath):
    data, meta = arff.loadarff(filepath)
    df = pd.DataFrame(data)
    df = df.applymap(lambda x: x.decode() if isinstance(x, bytes) else x)
    df = df.apply(pd.to_numeric, errors='coerce')
    df.dropna(inplace=True)
    return df

data_file = "Training_Dataset_Cleaned_Incremented.arff"
df = load_data(data_file)

# === 2. TÃ¡ch táº­p dá»¯ liá»‡u ===
X = df.drop(columns=["Result"])
y = df["Result"]

count_0 = 0
count_2 = 0

for value in y:
    if value == 0:
        count_0 += 1
    elif value == 2:
        count_2 += 1

print(f"ğŸ“Š Sá»‘ lÆ°á»£ng lá»«a Ä‘áº£o ( 0 ): {count_0}")
print(f"ğŸ“Š Sá»‘ lÆ°á»£ng KhÃ´ng lá»«a Ä‘áº£o ( 2 ): {count_2}")

# Váº½ biá»ƒu Ä‘á»“ trÃ²n
data = [count_0, count_2]
labels = ["Lá»«a Ä‘áº£o (0)", "KhÃ´ng lá»«a Ä‘áº£o (2)"]
colors = ["red", "green"]

plt.figure(figsize=(7, 7))
wedges, _, autotexts = plt.pie(data, autopct='%1.1f%%', colors=colors, startangle=90)

# Äá»‹nh dáº¡ng vÄƒn báº£n trong biá»ƒu Ä‘á»“
for text in autotexts:
    text.set_fontsize(12)
    text.set_color("black")

# ThÃªm chÃº thÃ­ch
plt.legend(wedges, [f"Lá»«a Ä‘áº£o (0) - {count_0}", f"KhÃ´ng lá»«a Ä‘áº£o (2) - {count_2}"], title="ChÃº thÃ­ch", loc="best")
plt.title("Tá»· lá»‡ lá»«a Ä‘áº£o vÃ  khÃ´ng lá»«a Ä‘áº£o")
plt.axis("equal")  # Äáº£m báº£o hÃ¬nh trÃ²n khÃ´ng bá»‹ mÃ©o
plt.savefig("fraud_pie_chart.png", dpi=300)
plt.show()


# === 3. Chia train / test (70% train, 30% test) ===
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ============================
#  ğŸ”¹ MÃ´ hÃ¬nh KNN
# ============================

k_values = range(1, 30)
accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k, metric='hamming')
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    accuracies.append(acc)

best_k = k_values[np.argmax(accuracies)]
knn_best = KNeighborsClassifier(n_neighbors=best_k, metric='hamming')
knn_best.fit(X_train, y_train)
y_pred_knn = knn_best.predict(X_test)

accuracy_knn = accuracy_score(y_test, y_pred_knn)
cm_knn = confusion_matrix(y_test, y_pred_knn)

print(f"ğŸ”¹ Äá»™ chÃ­nh xÃ¡c KNN (K={best_k}): {accuracy_knn:.5f}")
print("ğŸ“Œ Ma tráº­n nháº§m láº«n (KNN):\n", cm_knn)

# joblib.dump(knn_best, "knn_model.pkl")

plt.figure(figsize=(10, 5))
plt.plot(k_values, accuracies, marker='o', linestyle='dashed', color='b', label="Äá»™ chÃ­nh xÃ¡c KNN")
plt.axvline(x=best_k, color='r', linestyle='--', label=f"K tá»‘i Æ°u: {best_k}")
plt.text(best_k, max(accuracies), f"{best_k}", verticalalignment='bottom', horizontalalignment='right', fontsize=12, color='black')
plt.xlabel("Sá»‘ hÃ ng xÃ³m (k)")
plt.ylabel("Äá»™ chÃ­nh xÃ¡c")
plt.title("Chá»n k tá»‘i Æ°u cho thuáº­t toÃ¡n KNN")
plt.legend()
plt.grid(True)
plt.savefig("knn_accuracy.png", dpi=300)
# plt.show()

# ============================
#  ğŸ”¹ MÃ´ hÃ¬nh Naive Bayes
# ============================
nb_model = CategoricalNB()
nb_model.fit(X_train, y_train)
y_pred_nb = nb_model.predict(X_test)

accuracy_nb = accuracy_score(y_test, y_pred_nb)
cm_nb = confusion_matrix(y_test, y_pred_nb)

print(f"ğŸ”¹ Äá»™ chÃ­nh xÃ¡c Naive Bayes: {accuracy_nb:.5f}")
print("ğŸ“Œ Ma tráº­n nháº§m láº«n (Naive Bayes):\n", cm_nb)

# joblib.dump(nb_model, "naive_bayes_model.pkl")


# ============================
#  ğŸ”¹ MÃ´ hÃ¬nh Random Forest
# ============================

n_estimators_range = range(100, 201)
max_depth_range = range(2, 21)
min_samples_split_range = range(2, 21)
min_samples_leaf_range = range(1, 21)

# LÆ°u káº¿t quáº£ Ä‘á»™ chÃ­nh xÃ¡c
accuracy_results = {}

# ğŸ“Œ 1. Tá»‘i Æ°u sá»‘ lÆ°á»£ng cÃ¢y con (n_estimators)
accuracy_results["n_estimators"] = []
for n in n_estimators_range:
    acc = accuracy_score(y_test, RandomForestClassifier(n_estimators=n, random_state=42).fit(X_train, y_train).predict(X_test))
    accuracy_results["n_estimators"].append(acc)

best_n = n_estimators_range[np.argmax(accuracy_results["n_estimators"])]
plt.figure(figsize=(10, 5))
plt.plot(n_estimators_range, accuracy_results["n_estimators"], marker='o', linestyle='dashed', color='blue')
plt.axvline(x=best_n, color='r', linestyle='--', label=f"Tá»‘i Æ°u: {best_n}")
plt.text(best_n, max(accuracy_results["n_estimators"]), f"{best_n}", verticalalignment='bottom', horizontalalignment='right', fontsize=12, color='black')
plt.xlabel("Sá»‘ cÃ¢y con (n_estimators)")
plt.ylabel("Äá»™ chÃ­nh xÃ¡c")
plt.title("Tá»‘i Æ°u n_estimators")
plt.legend()
plt.grid(True)
plt.savefig("n_estimators.png", dpi=300)

# ğŸ“Œ 2. Tá»‘i Æ°u Ä‘á»™ sÃ¢u tá»‘i Ä‘a (max_depth)
accuracy_results["max_depth"] = []
for d in max_depth_range:
    acc = accuracy_score(y_test, RandomForestClassifier(n_estimators=best_n, max_depth=d, random_state=42).fit(X_train, y_train).predict(X_test))
    accuracy_results["max_depth"].append(acc)

best_d = max_depth_range[np.argmax(accuracy_results["max_depth"])]
plt.figure(figsize=(10, 5))
plt.plot(max_depth_range, accuracy_results["max_depth"], marker='o', linestyle='dashed', color='blue')
plt.axvline(x=best_d, color='r', linestyle='--', label=f"Tá»‘i Æ°u: {best_d}")
plt.text(best_d, max(accuracy_results["max_depth"]), f"{best_d}", verticalalignment='bottom', horizontalalignment='right', fontsize=12, color='black')
plt.xlabel("Äá»™ sÃ¢u tá»‘i Ä‘a (max_depth)")
plt.ylabel("Äá»™ chÃ­nh xÃ¡c")
plt.title("Tá»‘i Æ°u max_depth")
plt.legend()
plt.grid(True)
plt.savefig("max_depth.png", dpi=300)

# ğŸ“Œ 3. Tá»‘i Æ°u sá»‘ máº«u tá»‘i thiá»ƒu Ä‘á»ƒ chia nhÃ¡nh (min_samples_split)
accuracy_results["min_samples_split"] = []
for s in min_samples_split_range:
    acc = accuracy_score(y_test, RandomForestClassifier(n_estimators=best_n, max_depth=best_d, min_samples_split=s, random_state=42).fit(X_train, y_train).predict(X_test))
    accuracy_results["min_samples_split"].append(acc)

best_s = min_samples_split_range[np.argmax(accuracy_results["min_samples_split"])]
plt.figure(figsize=(10, 5))
plt.plot(min_samples_split_range, accuracy_results["min_samples_split"], marker='o', linestyle='dashed', color='blue')
plt.axvline(x=best_s, color='r', linestyle='--', label=f"Tá»‘i Æ°u: {best_s}")
plt.text(best_s, max(accuracy_results["min_samples_split"]), f"{best_s}", verticalalignment='bottom', horizontalalignment='right', fontsize=12, color='black')
plt.xlabel("Sá»‘ máº«u tá»‘i thiá»ƒu Ä‘á»ƒ chia nhÃ¡nh (min_samples_split)")
plt.ylabel("Äá»™ chÃ­nh xÃ¡c")
plt.title("Tá»‘i Æ°u min_samples_split")
plt.legend()
plt.grid(True)
plt.savefig("min_samples_split.png", dpi=300)

# ğŸ“Œ 4. Tá»‘i Æ°u sá»‘ máº«u tá»‘i thiá»ƒu Ä‘á»ƒ lÃ m nÃºt lÃ¡ (min_samples_leaf)
accuracy_results["min_samples_leaf"] = []
for l in min_samples_leaf_range:
    acc = accuracy_score(y_test, RandomForestClassifier(n_estimators=best_n, max_depth=best_d, min_samples_split=best_s, min_samples_leaf=l, random_state=42).fit(X_train, y_train).predict(X_test))
    accuracy_results["min_samples_leaf"].append(acc)

best_l = min_samples_leaf_range[np.argmax(accuracy_results["min_samples_leaf"])]
plt.figure(figsize=(10, 5))
plt.plot(min_samples_leaf_range, accuracy_results["min_samples_leaf"], marker='o', linestyle='dashed', color='blue')
plt.axvline(x=best_l, color='r', linestyle='--', label=f"Tá»‘i Æ°u: {best_l}")
plt.text(best_l, max(accuracy_results["min_samples_leaf"]), f"{best_l}", verticalalignment='bottom', horizontalalignment='right', fontsize=12, color='black')
plt.xlabel("Sá»‘ máº«u tá»‘i thiá»ƒu Ä‘á»ƒ lÃ m lÃ¡ (min_samples_leaf)")
plt.ylabel("Äá»™ chÃ­nh xÃ¡c")
plt.title("Tá»‘i Æ°u min_samples_leaf")
plt.legend()
plt.grid(True)
plt.savefig("min_samples_leaf.png", dpi=300)


# ğŸ“Œ Huáº¥n luyá»‡n mÃ´ hÃ¬nh tá»‘i Æ°u
rf_best = RandomForestClassifier(
    n_estimators=best_n, 
    max_depth=best_d, 
    min_samples_split=best_s, 
    min_samples_leaf=best_l,
    random_state=42
)
rf_best.fit(X_train, y_train)
y_pred_rf = rf_best.predict(X_test)

# TÃ­nh Ä‘á»™ chÃ­nh xÃ¡c cuá»‘i cÃ¹ng
accuracy_rf = accuracy_score(y_test, y_pred_rf)
cm_rf = confusion_matrix(y_test, y_pred_rf)

print(f"ğŸ”¹ Äá»™ chÃ­nh xÃ¡c Random Forest: {accuracy_rf:.5f}")
print("ğŸ“Œ Ma tráº­n nháº§m láº«n (Ramdom Forest):\n", cm_rf)

# ğŸ” Láº¥y giÃ¡ trá»‹ Ä‘á»™ quan trá»ng cá»§a thuá»™c tÃ­nh
feature_importances = rf_best.feature_importances_

# ğŸ“Š Sáº¯p xáº¿p Ä‘á»™ quan trá»ng giáº£m dáº§n
feature_names = X_train.columns if isinstance(X_train, pd.DataFrame) else [f"Feature {i}" for i in range(X_train.shape[1])]
sorted_idx = np.argsort(feature_importances)[::-1]  # Sáº¯p xáº¿p giáº£m dáº§n
sorted_features = [feature_names[i] for i in sorted_idx]
sorted_importances = feature_importances[sorted_idx]

# ğŸ“Š Váº½ biá»ƒu Ä‘á»“ Ä‘á»™ quan trá»ng cá»§a cÃ¡c thuá»™c tÃ­nh (tÃ­nh theo %)
plt.figure(figsize=(12, 6))
plt.barh(sorted_features, sorted_importances * 100, color="royalblue")  # NhÃ¢n vá»›i 100
plt.xlabel("Má»©c Ä‘á»™ quan trá»ng (%)")  # Äá»•i Ä‘Æ¡n vá»‹ trá»¥c x
plt.ylabel("Thuá»™c tÃ­nh")
plt.title("Äá»™ quan trá»ng cá»§a cÃ¡c thuá»™c tÃ­nh trong Random Forest (%)")
plt.gca().invert_yaxis()  # Äáº£o ngÆ°á»£c trá»¥c Ä‘á»ƒ thuá»™c tÃ­nh quan trá»ng nháº¥t á»Ÿ trÃªn cÃ¹ng
plt.grid(axis="x", linestyle="--", alpha=0.7)

# LÆ°u hÃ¬nh áº£nh
plt.savefig("feature_importance.png", dpi=300)

# Váº½ vÃ  lÆ°u ma tráº­n nháº§m láº«n cho KNN
plt.figure(figsize=(10, 8))
sns.heatmap(cm_knn, annot=True, fmt="d", cmap="Greens", xticklabels=["KhÃ´ng lá»«a Ä‘áº£o", "Lá»«a Ä‘áº£o"], 
            yticklabels=["KhÃ´ng lá»«a Ä‘áº£o", "Lá»«a Ä‘áº£o"])
plt.title(f"Ma tráº­n nháº§m láº«n - KNN (k={best_k})")
plt.xlabel("GiÃ¡ trá»‹ thá»±c táº¿")
plt.ylabel("GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n")
plt.savefig("confusion_matrix_knn.png", dpi=300)

# Váº½ vÃ  lÆ°u ma tráº­n nháº§m láº«n cho Naive Bayes
plt.figure(figsize=(10, 8))
sns.heatmap(cm_nb, annot=True, fmt="d", cmap="Oranges", xticklabels=["KhÃ´ng lá»«a Ä‘áº£o", "Lá»«a Ä‘áº£o"], 
            yticklabels=["KhÃ´ng lá»«a Ä‘áº£o", "Lá»«a Ä‘áº£o"])
plt.title("Ma tráº­n nháº§m láº«n - Naive Bayes")
plt.xlabel("GiÃ¡ trá»‹ thá»±c táº¿")
plt.ylabel("GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n")
plt.savefig("confusion_matrix_naive_bayes.png", dpi=300)

# Váº½ vÃ  lÆ°u ma tráº­n nháº§m láº«n cho Radom Forest
plt.figure(figsize=(10, 8))
sns.heatmap(cm_rf, annot=True, fmt="d", cmap="YlGnBu", xticklabels=["KhÃ´ng lá»«a Ä‘áº£o", "Lá»«a Ä‘áº£o"], 
            yticklabels=["KhÃ´ng lá»«a Ä‘áº£o", "Lá»«a Ä‘áº£o"])
plt.title("Ma tráº­n nháº§m láº«n - Random Forest")
plt.xlabel("GiÃ¡ trá»‹ thá»±c táº¿")
plt.ylabel("GiÃ¡ trá»‹ dá»± Ä‘oÃ¡n")
plt.savefig("confusion_matrix_random_forest.png", dpi=300)

plt.show()

# TÃ¬m Ä‘á»™ chÃ­nh xÃ¡c cao nháº¥t
best_accuracy = max(accuracy_knn, accuracy_nb, accuracy_rf)

if best_accuracy == accuracy_knn:
    joblib.dump(knn_best, "knn_model.pkl")
    print(f"ğŸ“Œ MÃ´ hÃ¬nh cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao nháº¥t: KNN (k={best_k}, {accuracy_knn:.5f}) Ä‘Ã£ Ä‘Æ°á»£c lÆ°u!")
elif best_accuracy == accuracy_nb:
    joblib.dump(nb_model, "naive_bayes_model.pkl")
    print(f"ğŸ“Œ MÃ´ hÃ¬nh cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao nháº¥t: Naive Bayes ({accuracy_nb:.5f}) Ä‘Ã£ Ä‘Æ°á»£c lÆ°u!")
else:
    joblib.dump(rf_best, "random_forest_model.pkl")
    print(f"ğŸ“Œ MÃ´ hÃ¬nh cÃ³ Ä‘á»™ chÃ­nh xÃ¡c cao nháº¥t: Random Forest ({accuracy_rf:.5f}) Ä‘Ã£ Ä‘Æ°á»£c lÆ°u!")
